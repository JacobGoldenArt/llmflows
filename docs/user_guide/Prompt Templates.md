## TL;DR

```python
from llmflows.llms import OpenAI
from llmflows.prompts import PromptTemplate

title_template = PromptTemplate("Generate a title for a {style} song about {topic}.")
title_prompt = title_template.get_prompt(style="hip-hop", topic="friendship")
print(title_prompt)

llm = OpenAI()
song_title = llm.generate(title_prompt)
print(song_title)

```

***

## Guide

So far, we have gone over LLMs in LLMFlow and saw how we can use two LLM classes - OpenAI and OpenAIChat. As we saw, the most critical inputs to LLMs are their prompts. For example, the OpenAI LLM generates its output based on an input prompt. OpenAIChat uses a system prompt to guide its behavior and a list of prompts (message history) to create a response.

In the previous two examples, we used string constants as prompts, but in some cases, prompts might need to change dynamically. Imagine you are building a web app that generates song titles. The user must provide the song's style and topic, and the app will return the title. In this case, we can't have a static prompt since we don't know what the user will request before they send the actual request.

Prompt templates are the second primary abstraction in LLMFlows. Prompt templates are string templates where the user can specify variables within the text that can be dynamically filled in.

!!! info
    The `PromptTemplate` class can be imported from `llmflows.prompts`

Prompt templates are strings containing variables defined with curly brackets:

```python
from llmflows.prompts import PromptTemplate

title_template = PromptTemplate("Generate a title for a {style} song about {topic}.")
```

Once a prompt template is defined an actual prompt can be generated by providing the required variables. Imagine the user wants
to generate a song title in a hip-hop style about friendship:

```python
title_prompt = title_template.get_prompt(style="hip-hop", topic="friendship")
print(title_prompt)
```

```commandline
"Generate a title for a hip-hop song about friendship"
```

!!! question

    Q: What happens if we don't provide all the variables?

    A: The prompt template will raise an exception specifying that there are missing variables.

Now that we have the actual prompt we can use it with the LLM

```python
llm = OpenAI()
song_title, _, _ = llm.generate(title_prompt)
print(song_title)
```

```commandline
"True to the Crew"
```

***
[:material-arrow-left: Previous: Chat LLMs](Chat LLMs.md){ .md-button }
[Next: Combining LLMs :material-arrow-right:](Combining LLMs.md){ .md-button }
