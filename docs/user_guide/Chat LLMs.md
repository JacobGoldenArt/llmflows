## TL;DR

```python
from llmflows.llms import OpenAIChat

llm = OpenAIChat(system_prompt="You are a useful assistant")

while True:
    user_message = input("You:")
    llm.add_message(user_message)
    llm_response, call_data, model_config = llm.generate()
    print(f"LLM: {llm_response}")

```
***
## Guide
In the previous section, we introduced the LLM abstraction, and we went through an example using the `OpenAI` class that utilizes the
chat completion API from OpenAI to generate text based on an input prompt. This section will cover a second type of LLM - the `OpenAIChat`. 

Chat LLMs gained popularity since the release of ChatGPT and the chat completions API from OpenAI. LLMFlows has a `OpenAIChat` class that is an interface for this API:

```python
from llmflows.llms import OpenAIChat
```

Regular LLMs like GPT-3 require just an input prompt to make a completion, but Chat LLMs work a bit differently. Instead of a prompt, `OpenAIChat` 
requires a list of messages representing a conversation history between a user and an assistant. This conversation history is sent to the model, and a new message is generated based on it.
In addition to the conversation history, `OpenAIChat` supports system messages (AKA system prompts) that set the behavior of the Chat LLM. 

```python
llm = OpenAIChat(system_prompt="You are a useful assistant")
```

!!! info
    OpenAI's chat compeltion API supports three message types:

    1. `system` (system message setting the behavior of the LLM)
    2. `user` (message by the user)
    3. `assistant` (response generated by the LLM as a response to the user message)

Like `OpenAI`, The `OpenAIChat` class provides `generate()` and `generate_async()` methods but it also has methods for managing conversation history like 
adding, removing and updating messages. For more information check the [OpenAIChat](openai_chat.md) section of our API reference.

We can easily use these methods to build a simple chatbot assistant with a few lines of code:

```python
while True:
    user_message = input("You:")
    llm.add_message(user_message)
    llm_response, call_data, model_config = llm.generate()
    print(f"LLM: {llm_response}")
```

```commandline
You: hey
LLM: Hello! How can I assist you today?
You: ...
```

In the snippet above, we read the user message, and then pass it to the conversation history of the `llm` object with the help of the `add_message()` method.
Then we use the `generate()` method to produce an answer from the Chat LLM.

!!! info
    LLMFlow automatically adds the responses to the conversation history by default, so the user doesn't need to manage this.


***
[:material-arrow-left: Previous: LLMs](LLMs.md){ .md-button }
[Next: Prompt Templates :material-arrow-right:](Prompt Templates.md){ .md-button }