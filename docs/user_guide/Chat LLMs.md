## TL;DR

```python
from llmflows.llms import OpenAIChat

llm = OpenAIChat(system_prompt="You are a useful assistant")

while True:
    user_message = input("You:")
    llm.add_message(user_message)
    llm_response, call_data, model_config = llm.generate()
    print(f"LLM: {llm_response}")

```
***
## Guide
In the previous section we introduced the LLM class and we went through an example using the `OpenAI` class that utilizes the
chat copmpletion API from OpenAI to generate text based on an input prompt. In this section we will cover a second type of LLM - the `OpenAIChat`. Chat LLMs have gained popularity 
since the release of ChatGPT and the chat completions API from OpenAI.

```python
from llmflows.llms import OpenAIChat
```

Regular LLMs like GPT-3 require just a prompt to make a completion but Chat LLMs work a bit differently. Instead of a system prompt `OpenAIChat` 
requires a list of messages whicih represents a conversation history between a user and an assistant. This conversation history is sent to the model and a new message is generated based on it.
In addition to the conversation history, `OpenAIChat` supports system messages (AKA system prompts) that sets the behavior of the Chat LLM. 

```python
llm = OpenAIChat(system_prompt="You are a useful assistant")
```

!!! info
    OpenAI's chat compeltion API supports three main message types:

    1. `system` (system message setting the behavior of the LLM)
    2. `user` (message by the user)
    3. `assistant` (response generated by the LLM as a response to the user message)

Having established how chat LLMs work, it is fair to say that using ChatLLMs is a task of managing a conversation history.
Like `OpenAI`, `OpenAIChat` provides `generate()` and `generate_async()` methods but it also has methods for messaging conversation history like 
adding, removing and updating messages. For more information check the [OpenAIChat](openai_chat.md) section of our API reference.
You can easily use these methods to build a chatbot assistant with a few lines of code:

```python
while True:
    user_message = input("You:")
    llm.add_message(user_message)
    llm_response, call_data, model_config = llm.generate()
    print(f"LLM: {llm_response}")
```

```commandline
You: hey
LLM: Hello! How can I assist you today?
You: ...
```

In the snippet above, we read the user message and then we pass it to the conversation history of the `llm` object with the help of the `add_message()` method.
Then we use the `generate()` method to produce an answer from the Chat LLM.

!!! info
    LLMFlow automatically adds the responses to the conversation history by default so the user doesn't need to manage this.


***
[Previous](LLMs.md){ .md-button }
[Next](Prompt Templates.md){ .md-button }